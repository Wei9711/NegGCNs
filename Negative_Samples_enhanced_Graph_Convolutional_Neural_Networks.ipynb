{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Negative Samples-enhanced Graph Convolutional Neural Networks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0t667tFIq2P"
      },
      "source": [
        "This is the code implementation of **NegGCNs**, which can be run directly on Google colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CvXk48loQUY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df5cbcab-7727-4315-eff2-cdbdb1bc5604"
      },
      "source": [
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install -q torch-geometric\n",
        "\n",
        "import torch\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.6MB 2.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.4MB 2.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 225kB 5.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 19.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 6.2MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmyK2cPukSgN"
      },
      "source": [
        "# 0. Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o39IsvvYayHt"
      },
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "def load_item_pop(X_train):\n",
        "    item_pop = list()\n",
        "    node_deg = dict()\n",
        "    dd = defaultdict(list)\n",
        "    #edge: <node1> <node2>\n",
        "    for edge in X_train:\n",
        "        #   <node2>       <node1> \n",
        "        dd[int(edge[1])].append(int(edge[0]))\n",
        "    for key in dd.keys():\n",
        "        item_pop.append(1)\n",
        "    #deg_sum : nodes number\n",
        "    deg_sum = np.sum(item_pop)\n",
        "    for key in dd.keys():\n",
        "        node_deg[key] = 1/deg_sum\n",
        "    return node_deg, dd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bca2xsCROO9Y"
      },
      "source": [
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "#DFS\n",
        "class Personalized():\n",
        "    def __init__(self, nx_G, mask, walks_num):\n",
        "        self.G = nx_G\n",
        "        self.mask = mask\n",
        "        self.walks_num = walks_num\n",
        "\n",
        "    # iterative version\n",
        "    # walks_num = 10\n",
        "    def dfs(self, start_node):\n",
        "        stack=[]\n",
        "        stack.append(start_node)\n",
        "        seen=set()\n",
        "        seen.add(start_node)\n",
        "        walks = []\n",
        "        mask_list = set(self.mask[start_node])\n",
        "        # print(\"mask_list\", mask_list)\n",
        "        while (len(stack)>0):\n",
        "            vertex=stack.pop()\n",
        "            # print(\"vertex\", vertex)\n",
        "            nodes=self.G[vertex]\n",
        "            # print(\"nodes\", nodes)\n",
        "            for w in nodes:\n",
        "                if w not in seen:\n",
        "                    stack.append(w)\n",
        "                    seen.add(w)\n",
        "            # print(\"stack\",stack)\n",
        "            # if vertex in mask_list:\n",
        "            #     pass\n",
        "            # else:\n",
        "            walks.append(vertex)\n",
        "            # print(\"walks\",walks)\n",
        "            if len(walks) >= self.walks_num:\n",
        "                break\n",
        "        return walks\n",
        "\n",
        "    # #recursiveche version\n",
        "    # def dfs(self, start_node, walks=[]):\n",
        "    #     walks.append(start_node)\n",
        "    #     mask_list = set(self.mask[start_node])\n",
        "    #     for w in self.G[start_node]:\n",
        "    #         if w not in walks:\n",
        "    #             if len(walks) >= self.walks_num:\n",
        "    #                 break\n",
        "                \n",
        "    #             if w in mask_list:\n",
        "    #                 pass\n",
        "    #             else:\n",
        "    #                 if w == start_node:\n",
        "    #                     pass\n",
        "    #                 else:\n",
        "    #                     dfs(G, w, walks)\n",
        "    #     return walks\n",
        "    \n",
        "    def intermediate(self):\n",
        "        i = 0 \n",
        "        candidate = defaultdict(list)\n",
        "        for node in self.G.nodes():\n",
        "            # print(node)\n",
        "            walk = self.dfs(node)\n",
        "            candidate[node].extend(walk)\n",
        "            # i=i+1\n",
        "            # if i==2:\n",
        "            #   break\n",
        "        return candidate\n",
        "\n",
        "\n",
        "def candidate_choose(nx_Graph, mask, walks_num):\n",
        "    G = Personalized(nx_Graph, mask, walks_num)\n",
        "    #defaultdict(<class 'list'>, {967: [1186, 1222, 1145, 1119, 1453, 1112, 1131, 1084, 1834, 1295]})\n",
        "    candidates = G.intermediate()\n",
        "    return candidates"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur8ljG9p5rIx"
      },
      "source": [
        "import torch\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import add_self_loops, degree\n",
        "from torch_scatter import gather_csr, scatter\n",
        "class GCNConv(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GCNConv, self).__init__(aggr='add')  # \"Add\" aggregation.\n",
        "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # x has shape [N, in_channels]\n",
        "        # edge_index has shape [2, E]\n",
        "\n",
        "        # Step 1: Add self-loops to the adjacency matrix.\n",
        "        #edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
        "\n",
        "        # Step 2: Linearly transform node feature matrix.\n",
        "        x = self.lin(x)\n",
        "\n",
        "        # Step 3-5: Start propagating messages.\n",
        "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)\n",
        "\n",
        "    def message(self, x_j, edge_index, size):\n",
        "        # x_j has shape [E, out_channels]\n",
        "        # edge_index has shape [2, E]\n",
        "\n",
        "        # Step 3: Normalize node features.\n",
        "        row, col = edge_index\n",
        "        deg = degree(row, size[0], dtype=x_j.dtype)  # [N, ]\n",
        "        deg_inv_sqrt = deg.pow(-0.5)   # [N, ]\n",
        "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "\n",
        "        return norm.view(-1, 1) * x_j\n",
        "            \n",
        "    def update(self, aggr_out):\n",
        "        # aggr_out has shape [N, out_channels]\n",
        "\n",
        "        # Step 5: Return new node embeddings.\n",
        "        return aggr_out"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHXP8N_iXKMR"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class NegaGCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels,candidates,start_given,q_1_dict,N_steps,node1,NegRate,index_selectindex):\n",
        "        super(NegaGCN, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
        "        self.negaconv1 = GCNConv(dataset.num_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n",
        "        self.negaconv2 = GCNConv(hidden_channels, dataset.num_classes)\n",
        "        self.candidates = candidates\n",
        "        self.start_given = start_given\n",
        "        self.q_1_dict = q_1_dict\n",
        "        self.N_steps = N_steps\n",
        "        self.node1 = node1\n",
        "        self.NegRate = NegRate\n",
        "        self.index_selectindex = index_selectindex\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        #---first layer----\n",
        "        posi_x1 = self.conv1(x, edge_index)\n",
        "        #nega_sample: perform negative sampling\n",
        "        #data type: list\n",
        "        nega_sample = self.negative_sampling(posi_x1,x, edge_index.shape[1])\n",
        "        \n",
        "        #get negative sampling node index\n",
        "        nega_sample = torch.LongTensor(nega_sample)\n",
        "        NegOut = edge_index[0,index_selectindex].unsqueeze(0)\n",
        "        NegIn = nega_sample[index_selectindex].unsqueeze(0)\n",
        "        NegaIndex = torch.cat((NegOut, NegIn), 0)\n",
        "\n",
        "        #Using negative index to perfom conv\n",
        "        nega_x1 = self.negaconv1(x, NegaIndex)\n",
        "        #positve - negative\n",
        "        final_x1 = posi_x1 - self.NegRate * nega_x1\n",
        "        final_x1 = final_x1.relu()\n",
        "        final_x1 = F.dropout(final_x1, p=0.5, training=self.training)\n",
        "\n",
        "        #---second layer----\n",
        "\n",
        "        posi_x2 = self.conv2(final_x1, edge_index)\n",
        "        nega_sample = self.negative_sampling(posi_x2,final_x1, edge_index.shape[1])\n",
        "\n",
        "        nega_sample = torch.LongTensor(nega_sample)\n",
        "        NegOut = edge_index[0,index_selectindex].unsqueeze(0)\n",
        "        NegIn = nega_sample[index_selectindex].unsqueeze(0)\n",
        "        NegaIndex = torch.cat((NegOut, NegIn), 0)\n",
        "\n",
        "        nega_x2 = self.negaconv2(final_x1, NegaIndex)\n",
        "        final_x2 = posi_x2 - self.NegRate * nega_x2\n",
        "        return final_x2\n",
        "    \n",
        "    def negative_sampling(self,posi,x,num_edges):\n",
        "        # distribution = [i/np.sum(distribution) for i in distribution]\n",
        "        if self.start_given is None:\n",
        "            start = np.random.choice(list(self.q_1_dict.keys()),num_edges)  # random init nodes \n",
        "        else:\n",
        "            start = self.start_given\n",
        "        #print(start)\n",
        "        count = 0\n",
        "        cur_state = start #index of input node\n",
        "        user_list = self.node1\n",
        "        walks = defaultdict(list)\n",
        "        generate_examples = list()\n",
        "        while True:\n",
        "            y_list = list()\n",
        "            q_probs_list = list()\n",
        "            #(1) Generate y ∼ q(y|X(t)) where the proposal distribution q is arbitrary chosen as long as positive everywhere.\n",
        "            q_probs_next_list = list()\n",
        "            count += 1\n",
        "            sample_num = np.random.random() \n",
        "            #print(sample_num)\n",
        "            if sample_num < 0.5: # sample proba < 0.5 negtive correlation q_probs = 0.01\n",
        "                #q_1_dict : 1/deg_sum                        len(cur_state)=batch size=512   Same probability for each point\n",
        "                y_list = np.random.choice(list(self.q_1_dict.keys()), len(cur_state), p=list(self.q_1_dict.values()))\n",
        "                #print(\"y_list\",y_list)\n",
        "                #q(i,j) Same probability for each point\n",
        "                q_probs_list = [self.q_1_dict[i] for i in y_list]\n",
        "                #print(\"q_probs_list\",q_probs_list)\n",
        "                #q(j,i) Same probability for each point\n",
        "                q_probs_next_list = [self.q_1_dict[i] for i in cur_state]\n",
        "                #print(\"q_probs_next_list\",q_probs_next_list)\n",
        "            else:\n",
        "                for i in cur_state:\n",
        "                    distribution = [1/len(self.candidates[i])] * len(self.candidates[i])\n",
        "                    #print('i',i)\n",
        "                    y = np.random.choice(self.candidates[i], 1, p=distribution)[0]\n",
        "                    #print(\"y\",y)\n",
        "                    y_list.append(y)\n",
        "                    index = self.candidates[i].index(y) # index of y in candidates[i]\n",
        "                    #print(\"index\",index)\n",
        "                    q_probs = distribution[index]\n",
        "                    #print(\"q_probs\",q_probs)\n",
        "                    q_probs_list.append(q_probs)\n",
        "                    node_list_next = self.candidates[y]\n",
        "                    #print(\"node_list_next\",node_list_next)\n",
        "                    if i in node_list_next:\n",
        "                        index_next = node_list_next.index(i)\n",
        "                        q_probs_next = distribution[index_next]\n",
        "                    else:\n",
        "                        q_probs_next = self.q_1_dict[i]\n",
        "                    q_probs_next_list.append(q_probs_next) \n",
        "                    #print(\"q_probs_next_list\",q_probs_next_list)\n",
        "\n",
        "            q_probs_list = torch.Tensor(q_probs_list)\n",
        "            q_probs_next_list = torch.Tensor(q_probs_next_list)\n",
        "            #print(y_list)\n",
        "            user_list_value = posi[user_list]\n",
        "            #print(user_list_value)\n",
        "            cur_state_value = posi[cur_state]\n",
        "            #print(cur_state_value)\n",
        "            y_list_value = posi[y_list]\n",
        "            #print(y_list_value)\n",
        "            p_probs = torch.sigmoid(torch.pow(torch.sum(user_list_value * y_list_value, axis=1), 0.25))\n",
        "            p_probs_next = torch.sigmoid(torch.pow(torch.sum(user_list_value * cur_state_value, axis=1), 0.25))\n",
        "            u = np.random.rand()\n",
        "            # print(\"u\",u)\n",
        "            p_probs = p_probs\n",
        "            p_probs_next = p_probs_next\n",
        "            A_a_list = (p_probs * q_probs_next_list)/(p_probs_next * q_probs_list)\n",
        "            A_a_list = A_a_list.detach().numpy()\n",
        "            # print(A_a_list)\n",
        "            next_state = list()\n",
        "            next_user = list()\n",
        "            # N_steps = 10 \n",
        "            if count > self.N_steps:\n",
        "                for i in list(range(len(cur_state))):\n",
        "                  walks[user_list[i]].append(y_list[i])\n",
        "   \n",
        "            else:# count < 10\n",
        "                for i in list(range(len(cur_state))):\n",
        "                  A_a = A_a_list[i]\n",
        "                  #   alpha                    \n",
        "                  alpha = min(1, A_a)\n",
        "                  if u < alpha:\n",
        "                    #accept\n",
        "                    next_state.append(y_list[i])\n",
        "                  else:\n",
        "                    next_state.append(cur_state[i])\n",
        "                cur_state = next_state\n",
        "                #print(\"next_state\", next_state)\n",
        "            length = 0\n",
        "            for key in walks.keys():\n",
        "                length += len(walks[key])\n",
        "            #print(length)\n",
        "\n",
        "            if length == num_edges:\n",
        "                # print(\"count\", count)\n",
        "                generate_examples = list()\n",
        "                for user in node1:\n",
        "                    d = walks[user]\n",
        "                    if len(d) == 1:\n",
        "                        generate_examples.append(d[0])    \n",
        "                    else:\n",
        "                        generate_examples.append(d[0])\n",
        "                        del walks[user][0]\n",
        "                break\n",
        "            else:\n",
        "                continue  \n",
        "        return generate_examples\n",
        "\n",
        "# def __init__(self, hidden_channels,candidates,start_given,q_1_dict,N_steps,node1):"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqZGLNgJKauo"
      },
      "source": [
        "# 1. Load datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDf2XiitofET",
        "outputId": "7b64324f-35f0-4638-a636-7b47f719de16"
      },
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "from torch_geometric.datasets import KarateClub\n",
        "\n",
        "dataset = Planetoid(root='data/Planetoid', name='Citeseer', transform=NormalizeFeatures())\n",
        "# dataset = Planetoid(root='data/Planetoid', name='Pubmed')\n",
        "#dataset = KarateClub()\n",
        "print()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('======================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "data = dataset[0]  # Get the first graph object.\n",
        "\n",
        "print()\n",
        "print(data)\n",
        "print('===========================================================================================================')\n",
        "\n",
        "# Gather some statistics about the graph.\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
        "print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')\n",
        "print(f'Contains self-loops: {data.contains_self_loops()}')\n",
        "print(f'Is undirected: {data.is_undirected()}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\n",
            "Processing...\n",
            "Done!\n",
            "\n",
            "Dataset: Citeseer():\n",
            "======================\n",
            "Number of graphs: 1\n",
            "Number of features: 3703\n",
            "Number of classes: 6\n",
            "\n",
            "Data(edge_index=[2, 9104], test_mask=[3327], train_mask=[3327], val_mask=[3327], x=[3327, 3703], y=[3327])\n",
            "===========================================================================================================\n",
            "Number of nodes: 3327\n",
            "Number of edges: 9104\n",
            "Average node degree: 2.74\n",
            "Number of training nodes: 120\n",
            "Training node label rate: 0.04\n",
            "Contains isolated nodes: True\n",
            "Contains self-loops: False\n",
            "Is undirected: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-zJjZ4zjhqO"
      },
      "source": [
        "# 2. Generate graphs to calculate q() and DFS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df3LX05Ra9v9"
      },
      "source": [
        "import random\n",
        "from torch_geometric.utils import add_self_loops,to_networkx\n",
        "G = to_networkx(data, to_undirected=True)\n",
        "\n",
        "edge_index, _ = add_self_loops(data.edge_index, num_nodes=data.x.size(0))\n",
        "# print(edge_index.shape)\n",
        "edge_index_list = torch.transpose(edge_index, 0, 1).tolist()\n",
        "# print(edge_index_list)\n",
        "node1 = [x[0] for x in edge_index_list] # out nodes\n",
        "node2 = [x[1] for x in edge_index_list] # out nodes\n",
        "# print(len(node1))\n",
        "# print(len(edge_index_list))\n",
        "# print(edge_index_list[0])\n",
        "q_1_dict, mask = load_item_pop(edge_index_list)\n",
        "\n",
        "walks_num = 10\n",
        "candidates = candidate_choose(G, mask, walks_num)\n",
        "# print(candidates[0])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZOOQtHhj1UK"
      },
      "source": [
        "# 3. Get sampling nodes number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z59luooOHV_P"
      },
      "source": [
        "rate = 0.9 #sampling rate\n",
        "select_index = []\n",
        "outnode = edge_index[0].numpy()\n",
        "for i in range(data.num_nodes):\n",
        "  temp = np.where(edge_index[0].numpy()==i)\n",
        "  index_EdgeIndex = np.squeeze(np.asarray(temp))\n",
        "  if np.ndim(index_EdgeIndex) == 0:\n",
        "    select_index.append(list(np.array(index_EdgeIndex, ndmin=1)))\n",
        "  else:\n",
        "    EdgeNumber = index_EdgeIndex.shape[0]\n",
        "    sampleNumber = int(EdgeNumber * rate)\n",
        "    # list1 = range(EdgeNumber)\n",
        "    # print(list1)\n",
        "    randIndex = random.sample(list(index_EdgeIndex), sampleNumber)\n",
        "    # print(randIndex)\n",
        "    select_index.append(randIndex)\n",
        "    # print(\"select_index\",select_index)\n",
        "# print(select_index)\n",
        "index_selectindex = sum(select_index, [])\n",
        "# print(index_selectindex)\n",
        "# print(len(index_selectindex))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tHryf-7kDvV"
      },
      "source": [
        "# 4. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RJSBbkd3Qda"
      },
      "source": [
        "def test():\n",
        "      model.eval()\n",
        "      out = model(data.x, edge_index)\n",
        "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
        "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
        "      return test_acc\n",
        "\n",
        "\n",
        "model = NegaGCN(hidden_channels=16,\n",
        "          candidates=candidates,\n",
        "          start_given=None,\n",
        "          q_1_dict=q_1_dict,\n",
        "          N_steps=10,\n",
        "          node1=node1,\n",
        "          NegRate = 1.25,\n",
        "          index_selectindex = index_selectindex)\n",
        "\n",
        "# model = GCN(hidden_channels=16)\n",
        "\n",
        "print(model)\n",
        "\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# data = data.to(device)\n",
        "# edge_index = edge_index.to(device)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "for epoch in range(1, 201):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()  # Clear gradients.\n",
        "    out = model(data.x, edge_index)  # Perform a single forward pass.\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
        "    loss.backward()  # Derive gradients.\n",
        "    optimizer.step()\n",
        "    \n",
        "    test_acc = test()\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f},Test Accuracy: {test_acc:.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}